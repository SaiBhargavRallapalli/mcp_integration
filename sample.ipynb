{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up environment\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import StructuredTool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.session import ClientSession\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"app.log\", level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables\n",
    "assert os.getenv(\"GROQ_API_KEY\"), \"GROQ_API_KEY not set\"\n",
    "# assert os.getenv(\"HOROSCOPE_ENDPOINT\"), \"HOROSCOPE_ENDPOINT not set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCP tools from JSON configuration\n",
    "async def load_mcp_tools(config_path: str):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    all_tools = []\n",
    "    for server_name, server_config in config[\"mcpServers\"].items():\n",
    "        async with streamablehttp_client(server_config[\"url\"]) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                await session.initialize()\n",
    "                tools = await session.list_tools()\n",
    "                for tool in tools:\n",
    "                    async def call_tool(params: dict, session=session, tool_name=tool[\"name\"]):\n",
    "                        return await session.call_tool(tool_name, params)\n",
    "                    schema = {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\"type\": \"string\"},\n",
    "                            \"zodiac_sign\": {\"type\": \"string\"},\n",
    "                            \"horoscope_type\": {\"type\": \"string\", \"enum\": [\"DAILY\", \"MONTHLY\"]}\n",
    "                        },\n",
    "                        \"required\": [\"zodiac_sign\"] if tool[\"name\"] == \"get_horoscope\" else [\"query\"]\n",
    "                    }\n",
    "                    all_tools.append(\n",
    "                        StructuredTool.from_function(\n",
    "                            func=call_tool,\n",
    "                            name=tool[\"name\"],\n",
    "                            description=tool[\"description\"],\n",
    "                            args_schema=schema\n",
    "                        )\n",
    "                    )\n",
    "    return all_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LangGraph state and workflow\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage | ToolMessage], operator.add]\n",
    "    tools_called: List[str]\n",
    "\n",
    "async def setup_llm_and_tools():\n",
    "    tools = await load_mcp_tools(\"mcp_config.json\")\n",
    "    llm = ChatGroq(model=\"llama3-8b-8192\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    return llm.bind_tools(tools), tools\n",
    "\n",
    "async def llm_node(state: GraphState, llm):\n",
    "    logging.info(f\"Processing messages: {[msg.content for msg in state['messages']]}\")\n",
    "    response = await llm.ainvoke(state[\"messages\"])\n",
    "    tools_called = state.get(\"tools_called\", []) + [call[\"name\"] for call in response.tool_calls]\n",
    "    return {\"messages\": [response], \"tools_called\": tools_called}\n",
    "\n",
    "async def tool_node(state: GraphState, tools):\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    results = []\n",
    "    for call in tool_calls:\n",
    "        tool = next(t for t in tools if t.name == call[\"name\"])\n",
    "        logging.info(f\"Invoking tool: {call['name']} with args: {call['args']}\")\n",
    "        result = await tool.ainvoke(call[\"args\"])\n",
    "        results.append(ToolMessage(content=str(result), tool_call_id=call[\"id\"]))\n",
    "    return {\"messages\": results}\n",
    "\n",
    "def should_continue(state: GraphState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    return \"continue\" if hasattr(last_message, \"tool_calls\") and last_message.tool_calls else \"end\"\n",
    "\n",
    "async def create_workflow():\n",
    "    llm, tools = await setup_llm_and_tools()\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"llm\", lambda state: llm_node(state, llm))\n",
    "    workflow.add_node(\"tools\", lambda state: tool_node(state, tools))\n",
    "    workflow.set_entry_point(\"llm\")\n",
    "    workflow.add_conditional_edges(\"llm\", should_continue, {\"continue\": \"tools\", \"end\": END})\n",
    "    workflow.add_edge(\"tools\", \"llm\")\n",
    "    app = workflow.compile()\n",
    "    print(app)\n",
    "    return workflow.compile(), llm, tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/saibhargavrallapalli/Documents/Git/mcp_integration/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/2450133448.py\", line 15, in <module>\n",
      "  |     result = await run_prompt(prompt)\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/2450133448.py\", line 3, in run_prompt\n",
      "  |     app, _, _ = await create_workflow()\n",
      "  |                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/3753619681.py\", line 32, in create_workflow\n",
      "  |     llm, tools = await setup_llm_and_tools()\n",
      "  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/3753619681.py\", line 7, in setup_llm_and_tools\n",
      "  |     tools = await load_mcp_tools(\"mcp_config.json\")\n",
      "  |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/1786243658.py\", line 8, in load_mcp_tools\n",
      "  |     async with streamablehttp_client(server_config[\"url\"]) as (read, write):\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/saibhargavrallapalli/Documents/Git/mcp_integration/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 464, in streamablehttp_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/saibhargavrallapalli/Documents/Git/mcp_integration/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/saibhargavrallapalli/Documents/Git/mcp_integration/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 492, in streamablehttp_client\n",
      "    |     yield (\n",
      "    |   File \"/var/folders/n2/2s0tjc891jx80m_1lwcfhm_40000gn/T/ipykernel_89285/1786243658.py\", line 8, in load_mcp_tools\n",
      "    |     async with streamablehttp_client(server_config[\"url\"]) as (read, write):\n",
      "    |                                                               ^^^^^^^^^^^^^\n",
      "    | ValueError: too many values to unpack (expected 2)\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the workflow with sample prompts\n",
    "async def run_prompt(prompt: str):\n",
    "    app, _, _ = await create_workflow()\n",
    "    result = await app.ainvoke({\"messages\": [HumanMessage(content=prompt)], \"tools_called\": []})\n",
    "    logging.info(f\"Tools called: {result['tools_called']}\")\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "# Run prompts (Jupyter handles async automatically with IPython)\n",
    "prompts = [\n",
    "    \"What's the daily horoscope for Virgo?\",\n",
    "    \"What's the monthly horoscope for Leo?\",\n",
    "    \"Search for recent AI news\"\n",
    "]\n",
    "for prompt in prompts:\n",
    "    result = await run_prompt(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
